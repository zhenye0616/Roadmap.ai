{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: playwright in /Users/zhenye/opt/miniconda3/envs/ai_roadmap/lib/python3.9/site-packages (1.51.0)\n",
      "Requirement already satisfied: pyee<13,>=12 in /Users/zhenye/opt/miniconda3/envs/ai_roadmap/lib/python3.9/site-packages (from playwright) (12.1.1)\n",
      "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in /Users/zhenye/opt/miniconda3/envs/ai_roadmap/lib/python3.9/site-packages (from playwright) (3.2.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/zhenye/opt/miniconda3/envs/ai_roadmap/lib/python3.9/site-packages (from pyee<13,>=12->playwright) (4.13.2)\n"
     ]
    }
   ],
   "source": [
    "%%python -m pip install playwright\n",
    "%%playwright install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from playwright.async_api import async_playwright\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def extract_linkedin_job_async(url):\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(url)\n",
    "        \n",
    "        await page.wait_for_timeout(5000)  # Wait 5 seconds for the page to fully load\n",
    "\n",
    "        html = await page.content()\n",
    "        await browser.close()\n",
    "        \n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    job_info = {}\n",
    "\n",
    "    # Title\n",
    "    title = soup.find('h1')\n",
    "    job_info['title'] = title.get_text(strip=True) if title else None\n",
    "\n",
    "    # Company\n",
    "    company = soup.find('a', class_='topcard__org-name-link')\n",
    "    if not company:\n",
    "        company = soup.find('span', class_='topcard__flavor')\n",
    "    job_info['company'] = company.get_text(strip=True) if company else None\n",
    "\n",
    "    # Location\n",
    "    location = soup.find('span', class_='topcard__flavor topcard__flavor--bullet')\n",
    "    job_info['location'] = location.get_text(strip=True) if location else None\n",
    "\n",
    "    # Employment Type and Industries\n",
    "    job_info['employment_type'] = None\n",
    "    job_info['industries'] = None\n",
    "    criteria_blocks = soup.find_all('li', class_='description__job-criteria-item')\n",
    "    for block in criteria_blocks:\n",
    "        header = block.find('h3').get_text(strip=True).lower()\n",
    "        value = block.find('span').get_text(strip=True)\n",
    "        if 'employment type' in header:\n",
    "            job_info['employment_type'] = value\n",
    "        if 'industries' in header:\n",
    "            job_info['industries'] = value\n",
    "\n",
    "    # Full Job Description\n",
    "    description_block = soup.find('div', class_='show-more-less-html__markup')\n",
    "    job_info['full_description'] = description_block.get_text(separator=\"\\n\", strip=True) if description_block else None\n",
    "\n",
    "    return job_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_job_description(full_description):\n",
    "    \"\"\"\n",
    "    Split LinkedIn job description into structured sections.\n",
    "    \"\"\"\n",
    "    sections = {\n",
    "        \"responsibilities\": None,\n",
    "        \"required_qualifications\": None,\n",
    "        \"preferred_qualifications\": None,\n",
    "        \"other\": None,\n",
    "    }\n",
    "    \n",
    "    # Normalize\n",
    "    text = full_description.replace(\"\\r\", \"\").strip()\n",
    "\n",
    "    # Define simple section headings to split on\n",
    "    patterns = {\n",
    "        \"responsibilities\": r\"(responsibilities|what you'll do|duties)\",\n",
    "        \"required_qualifications\": r\"(required qualifications|basic qualifications|must have|requirements)\",\n",
    "        \"preferred_qualifications\": r\"(preferred qualifications|nice to have|would be great if)\",\n",
    "    }\n",
    "\n",
    "    # Find split points\n",
    "    split_points = {}\n",
    "    for section, pattern in patterns.items():\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            split_points[match.start()] = section\n",
    "\n",
    "    if not split_points:\n",
    "        # Nothing matched, fallback\n",
    "        sections[\"other\"] = text\n",
    "        return sections\n",
    "\n",
    "    # Sort split points\n",
    "    sorted_points = sorted(split_points.items())\n",
    "    sorted_points.append((len(text), None))  # Add end of text\n",
    "\n",
    "    # Extract sections\n",
    "    for idx in range(len(sorted_points) - 1):\n",
    "        start_idx, section = sorted_points[idx]\n",
    "        end_idx, _ = sorted_points[idx + 1]\n",
    "        extracted_text = text[start_idx:end_idx].strip()\n",
    "\n",
    "        if section:\n",
    "            sections[section] = extracted_text\n",
    "\n",
    "    return sections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# async def main():\n",
    "#     url = \"https://www.linkedin.com/jobs/view/4208690456\"\n",
    "#     job_data = await extract_linkedin_job_async(url)\n",
    "#     for k, v in job_data.items():\n",
    "#         print(f\"{k}: {v}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['title', 'company', 'location', 'employment_type', 'industries', 'responsibilities', 'required_qualifications', 'preferred_qualifications', 'other'])\n",
      "title: Data Analyst\n",
      "\n",
      "company: RPL International\n",
      "\n",
      "location: Alaska, United States\n",
      "\n",
      "employment_type: Part-time\n",
      "\n",
      "industries: Staffing and Recruiting, Advertising Services, and Government Administration\n",
      "\n",
      "responsibilities: Responsibilities:\n",
      "Collect, clean, and analyze data from multiple sources to support business initiatives\n",
      "Build dashboards, reports, and visualizations to communicate insights effectively\n",
      "Identify trends, patterns, and anomalies in data and present actionable recommendations\n",
      "Support client projects with ad-hoc data analysis and reporting\n",
      "Collaborate with internal teams (Finance, Operations, Marketing) to understand data needs\n",
      "Maintain and document datasets and analytical processes\n",
      "Qualifications:\n",
      "Bachelorâ€™s degree in Data Science, Statistics, Computer Science, Economics, or related field\n",
      "2+ years of experience in a data analyst or similar role\n",
      "Proficiency in Excel, SQL, and at least one data visualization tool (e.g., Tableau, Power BI)\n",
      "Familiarity with Python or R is a plus\n",
      "Strong analytical, problem-solving, and organizational skills\n",
      "Excellent verbal and written communication skills\n",
      "Self-motivated with the ability to manage time and priorities in a remote work environment\n",
      "What We Offer:\n",
      "Flexible working hours (20 hours/week)\n",
      "100% remote work environment\n",
      "Competitive hourly compensation\n",
      "Opportunities for growth and development\n",
      "A collaborative and supportive team culture\n",
      "\n",
      "required_qualifications: None\n",
      "\n",
      "preferred_qualifications: None\n",
      "\n",
      "other: None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Scrape full_description\n",
    "#url = \"https://www.linkedin.com/jobs/view/4208690456\"\n",
    "url = 'https://www.linkedin.com/jobs/view/4215657027/?alternateChannel=search&refId=l0Y1nQXsuWT3weM4a3zs%2Fw%3D%3D&trackingId=3iqJ%2Fqp2QBpUbhWpR7Nufg%3D%3D'\n",
    "job_data = await extract_linkedin_job_async(url)\n",
    "\n",
    "# Step 2: Split full_description into sections\n",
    "if job_data.get(\"full_description\"):\n",
    "    split_sections = split_job_description(job_data[\"full_description\"])\n",
    "    job_data.update(split_sections)\n",
    "\n",
    "# Step 3: (Optional) Remove full_description if you only want clean fields\n",
    "del job_data[\"full_description\"]\n",
    "\n",
    "print(job_data.keys())\n",
    "# Now job_data looks clean\n",
    "for k, v in job_data.items():\n",
    "    print(f\"{k}: {v}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skillgap-backend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
